{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruoyuz31/Coding-Practice/blob/main/TPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA0rETRQZBON"
      },
      "source": [
        "# EE 508 Reading Assignment 2:\n",
        "You can access paper using this [link](https://arxiv.org/pdf/1704.04760.pdf). Please refer to this [guideline](http://ccr.sigcomm.org/online/files/p83-keshavA.pdf) on how to read a research paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-HWWv6kaxrx"
      },
      "source": [
        "### Q1: (1 Point)\n",
        "\n",
        "**Disallowed** list:\n",
        "- You **MAY NOT** collaborate with anyone else on this assignment. This means you cannot talk to anyone else about the assignment until after deadline.\n",
        "- You **MAY NOT** use ChatGPT and services like that\n",
        "\n",
        "**Allowed** list:\n",
        "- Notes including any slides from the class\n",
        "- The textbooks\n",
        "- The given paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz0uD-nHbHuT"
      },
      "source": [
        "### A1:\n",
        "\n",
        "I affirm I have read these exam rules and will follow them. Failure to do so may subject me to sanctions including an F in the course.\n",
        "\n",
        "**Type your full name to affirm you have read the above statement:** Ruoyu Zhao\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzNWLVCLbcjq"
      },
      "source": [
        "----\n",
        "### Q2 Summary (24 Points):\n",
        "\n",
        "Summarize the main objectives and contributions of the paper.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j5A4_Zfbpra"
      },
      "source": [
        "### A2:\n",
        "This paper's objective is to analyze the performance of Tensor Processing Unit in datacenter and compare it with the performance of GPU and CPU.\n",
        "\n",
        "The contributions are as follows:\n",
        "1. Introduced the structure and operation scheme of TPUs.\n",
        "2. Measured the inference performance of TPU, GPU and CPU on 2 LSTM models, 2 MLP models and 2 CNN models.\n",
        "3. Compared the performances of TPU, GPU and CPU from different aspects, including response time, throughput, cost-performance and energy proportionality.\n",
        "4. Use simulation to predict the potential improvements of TPU design."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE2n5UQHbuZ-"
      },
      "source": [
        "---\n",
        "### Q3 Comprehension (15 Points):\n",
        "- What is a Tensor Processing Unit (TPU) and how does it differ from traditional CPUs and GPUs in terms of design and purpose?\n",
        "- Why is there a need for specialized hardware like TPUs in modern data centers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O7k2DbXcNp0"
      },
      "source": [
        "### A3:\n",
        "\n",
        "\n",
        "\n",
        "*   1:\n",
        "\n",
        "TPU is a custom ASIC designed to accelerate the inference of machine learning models.\n",
        "\n",
        "Regarding the design, comparing to CPU, TPU has large number of coprocesssors to run at the same time while CPU only has a few coprocessors. TPU has large size of memory buffers to access to weight values in a faster speed while CPU only has a smaller number of registers to access immeditely. Comparing to both GPU and CPU, TPU quantize the input values into integers for multiplication while GPU and CPU do floating-point calculation. TPU use special firmware to divid and assign the specific machine learning models to run the full workload of all the cores while GPU and CPU are more generalized for all the tasks and cannot realize full optimization on mahine learning tasks.\n",
        "\n",
        "Regarding the purpose, TPU is specifically for accelerating the inference of deep neural network models, while CPU and GPU are for general-purpose calculations.\n",
        "\n",
        "*  2:\n",
        "\n",
        "Deep neural networks has shown great capability to improve a wide range of problems, such as speech and image recognition. So just by accelerating deep neural networks, a lot of tasks can be accelerated. Besides, there is great need to achieve fast response time of model inference process during actual datacenter usages, so it makes sense to develop accelerators like TPU to focus on accelerating the inference speed and reduce its energy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnzcxvhWckY5"
      },
      "source": [
        "---\n",
        "### Q4 Technical Deep Dive (15 Points):\n",
        "- Describe the architecture of the TPU. How is it optimized for machine learning workloads?\n",
        "- How does the memory hierarchy of the TPU differ from traditional processors, and why is this significant for tensor computations?\n",
        "- What are systolic arrays, and why are they used in TPUs?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sv3rFNjpcuID"
      },
      "source": [
        "### A4:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1.   \n",
        "\n",
        "TPU is composed of a unified buffer to store inputs and outputs and transfer data with external memory, a matrix multiply unit with 256x256 MACs for 8-bit multiply-and-adds on integers, an accumulator to collect the products, an activation unit to perform activation operations of artificial neurons in neural networks, a normalize/pool unit to performance normalization and pooling operations, a weight FIFO to reads weight data from DRAMs.\n",
        "\n",
        "It is optmized for machine learning workloads by several methods. First, the large number of MACs achieve parallel computing during matrix multiplication. Second, the Unified Buffer realize parallel inputs and outputs to increase throughput. Third, the weight FIFO reduce the time limitation for weight extractions. Fourth, the specialized activation and normalize/pool units accelerate and parallelize the activation, normalization and pooling operations in machine learning models.\n",
        "\n",
        "2.   \n",
        "\n",
        "TPU has more on-chip memories for storing the inputs and outputs near the processors, while traditional processors has less memory close to the processors. This lead to a lot larger memory bandwidth of TPU, and extend its memory bound to run many machine leanring models.\n",
        "\n",
        "3.\n",
        "\n",
        "\n",
        "The systolic array load inputs from one direction, the unified buffer, and weights from another direction, weight FIFO, at a regular interval to calculate and accumulate the partial sums for final results.\n",
        "\n",
        "This save energy by reducing the number of times to read and writes the Unified Buffer, achieve high speed due to parallellism.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRBvFutTc2uh"
      },
      "source": [
        "---\n",
        "### Q5 Evaluation (15 Points):\n",
        "- How does the TPU's performance compare to contemporary CPUs and GPUs for machine learning tasks?\n",
        "- What benchmarks or workloads were used to evaluate the TPU's performance in the datacenter?\n",
        "- Discuss the TPU's performance per watt. Why is energy efficiency crucial in datacenter environments?\n",
        "- What types of machine learning models and tasks are best suited for TPUs?\n",
        "- How have TPUs impacted the training times of large-scale models at Google?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKUL8nhTdNLZ"
      },
      "source": [
        "### A5:\n",
        "\n",
        "\n",
        "\n",
        "1.   \n",
        "\n",
        "For the geometric mean of relative performance (including overdead) on all the tested machine learnign tasks TPU is 14.5 times as fast as the CPU and 13.2 times as fast as the GPU. If considering the actual model inference portion within datacenter to get the weighted mean of the relavant performance, then TPU is 29.2x of CPU and 15.3x of GPU.\n",
        "\n",
        "2.  \n",
        "\n",
        "The CPU platform is an 18-core, dual socket Haswell processor from Intel. The GPU platform is eight NVIDIA K80 cards. The workloads are six tasks. One DNN is RankBrain, one LSTM is a subset of SNM Translate, on CNN is inception, antoher CNN is DeepMind AlphaGo. However, there is still one DNN and one LSTM unknown.\n",
        "\n",
        "3.\n",
        "\n",
        "\n",
        "TPU has 17 to 34 times better total-performance/Watt than Haswell and 14 to 16 times total-performance /Watt than K80. Besides, TPU has 41 to 83 times incremental-performance/Watt than CPU and 25 to 29 times the incremental-performance/Watt than GPU.\n",
        "\n",
        "The energy effciency is crucial in datacenter because the power is correlated with total cost of ownership (TCO), and TCO is the best cost metric in a datacenter.\n",
        "\n",
        "\n",
        "4.\n",
        "\n",
        "\n",
        "TPU is best suited for the models with deep feature depths to enable more MACs run at the same time and avoid the MACs from waiting for weights to load frequently.\n",
        "\n",
        "Besides, TPU is more suitable to run models that can fully utilize its workload, such as with larger size of matrix multiplication at the same time, in order to keep it working with full load. This is because its energy proportionality with portional loads is high comparing to GPU and CPU.\n",
        "\n",
        "5.\n",
        "\n",
        "TPU is only used for inference and it has no influence on the training time of models.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp_FV2bzdV4D"
      },
      "source": [
        "---\n",
        "### Q6 Contextual Understanding (10 Points):\n",
        "- What are some of the challenges faced in deploying TPUs in datacenters?\n",
        "- Are there specific machine learning tasks or models that TPUs might struggle with compared to other hardware?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvpxYhvNdg18"
      },
      "source": [
        "### A6:\n",
        "\n",
        "1.\n",
        "\n",
        "\n",
        "The design of memory communication within datacenters needs to be improved in order to meet the high memory throughput of the TPUs.\n",
        "\n",
        "Task management in datacenters need to be carefully developed to fully utilize the TPU workloads, or the TPU efficiency is significantly reduced.\n",
        "\n",
        "\n",
        "2.\n",
        "\n",
        "\n",
        "TPU might struggle with deploying small DNNs like AlexNet or VGG since each of the layers have shallow feature depth which cannot fully utilize the TPU wordloads."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxUpnGmtdjfX"
      },
      "source": [
        "---\n",
        "### Q7 Discussion and Critique (10 points):\n",
        "- What are the strengths and weaknesses of the paper's methodology and analysis?\n",
        "- Are there any potential biases or assumptions in the paper that you disagree with or find questionable?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjAQy1qLdvZ7"
      },
      "source": [
        "### A7:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSMCnPg8d_0S"
      },
      "source": [
        "---\n",
        "### Q8 Reflection (10 Points):\n",
        "- How do TPUs fit into the larger trend of custom hardware accelerators for machine learning, such as FPGAs and ASICs?\n",
        "- Discuss the trade-offs between general-purpose hardware (like CPUs) and specialized hardware (like TPUs) in the context of evolving machine learning workloads.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBtgEgPXeNR5"
      },
      "source": [
        "### A8:"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}